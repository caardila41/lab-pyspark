# GuÃ­a de EjecuciÃ³n - Data Engineering with Databricks Cookbook

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto es un entorno completo de **Data Engineering** que incluye Apache Spark, Delta Lake, Kafka, y JupyterLab para aprender y practicar tÃ©cnicas de ingenierÃ­a de datos. El entorno estÃ¡ completamente containerizado usando Docker.

## ğŸ—ï¸ Arquitectura del Sistema

El entorno incluye los siguientes componentes:

- **Apache Spark Cluster**: Cluster distribuido con master y workers
- **JupyterLab**: Entorno de desarrollo con notebooks
- **Kafka**: Para streaming de datos
- **Zookeeper**: Coordinador para Kafka
- **Delta Lake**: Para tablas ACID y time travel

### Componentes Docker:

1. **spark-master**: Nodo maestro del cluster Spark (puerto 8080)
2. **spark-worker-1**: Worker 1 del cluster Spark (puerto 8081)
3. **spark-worker-2**: Worker 2 del cluster Spark (puerto 8082)
4. **jupyterlab**: Entorno de desarrollo (puerto 8888)
5. **kafka**: Broker de mensajerÃ­a (puerto 9092)
6. **zookeeper**: Coordinador de Kafka (puerto 2181)

## ğŸš€ Requisitos Previos

### Software Requerido:

| Software | VersiÃ³n MÃ­nima | DescripciÃ³n |
|----------|----------------|-------------|
| Docker Engine | 18.02.0+ | Motor de contenedores |
| Docker Compose | 1.25.5+ | Orquestador de contenedores |
| Git | Cualquier versiÃ³n | Control de versiones |

### Requisitos de Hardware:

- **RAM**: MÃ­nimo 4GB, recomendado 8GB+
- **CPU**: 2 cores mÃ­nimo, 4+ recomendado
- **Disco**: 10GB de espacio libre

## ğŸ“¦ InstalaciÃ³n y ConfiguraciÃ³n

### Paso 1: Clonar el Repositorio

```bash
git clone <URL_DEL_REPOSITORIO>
cd Data-Engineering-with-Databricks-Cookbook
```

### Paso 2: Construir las ImÃ¡genes Docker

**En Windows (PowerShell):**
```powershell
# Navegar al directorio del proyecto
cd Data-Engineering-with-Databricks-Cookbook

# Ejecutar el script de construcciÃ³n
./build.sh
```

**En Linux/Mac:**
```bash
# Dar permisos de ejecuciÃ³n al script
chmod +x build.sh

# Ejecutar el script de construcciÃ³n
./build.sh
```

**Nota**: Si tienes problemas con el script `build.sh`, puedes ejecutar los comandos manualmente:

```bash
# Limpiar contenedores e imÃ¡genes existentes
docker system prune -a

# Construir las imÃ¡genes en orden
docker build -f docker/base/Dockerfile -t base:latest .
docker build -f docker/spark-base/Dockerfile -t spark-base:3.4.1 .
docker build -f docker/spark-master/Dockerfile -t spark-master:3.4.1 .
docker build -f docker/spark-worker/Dockerfile -t spark-worker:3.4.1 .
docker build -f docker/jupyterlab/Dockerfile -t jupyterlab:4.0.2-spark-3.4.1 .
```

### Paso 3: Iniciar el Entorno

```bash
# Iniciar todos los servicios
docker-compose up -d
```

### Paso 4: Verificar que Todo Funciona

```bash
# Verificar que todos los contenedores estÃ¡n ejecutÃ¡ndose
docker-compose ps
```

DeberÃ­as ver algo como:
```
NAME            COMMAND                  SERVICE             STATUS              PORTS
jupyterlab      "jupyter lab --ip=0.â€¦"   jupyterlab          Up                  0.0.0.0:8888->8888/tcp, 0.0.0.0:4040->4040/tcp
kafka           "/opt/bitnami/scriptâ€¦"   kafka               Up                  0.0.0.0:9092->9092/tcp
spark-master    "bin/spark-class orgâ€¦"   spark-master        Up                  0.0.0.0:7077->7077/tcp, 0.0.0.0:8080->8080/tcp
spark-worker-1  "bin/spark-class orgâ€¦"   spark-worker-1      Up                  0.0.0.0:8081->8081/tcp
spark-worker-2  "bin/spark-class orgâ€¦"   spark-worker-2      Up                  0.0.0.0:8082->8081/tcp
zookeeper       "/opt/bitnami/scriptâ€¦"   zookeeper           Up                  0.0.0.0:2181->2181/tcp
```

## ğŸŒ Acceso a los Servicios

Una vez que el entorno estÃ© ejecutÃ¡ndose, puedes acceder a los siguientes servicios:

### 1. JupyterLab (Entorno Principal)
- **URL**: http://localhost:8888
- **DescripciÃ³n**: Entorno de desarrollo con notebooks
- **Sin contraseÃ±a**: El token estÃ¡ deshabilitado para facilitar el acceso

### 2. Spark Master UI
- **URL**: http://localhost:8080
- **DescripciÃ³n**: Interfaz web para monitorear el cluster Spark

### 3. Spark Worker UIs
- **Worker 1**: http://localhost:8081
- **Worker 2**: http://localhost:8082
- **DescripciÃ³n**: Monitoreo de los workers individuales

### 4. Spark Application UI
- **URL**: http://localhost:4040
- **DescripciÃ³n**: Interfaz para aplicaciones Spark activas

## ğŸ“š Estructura del Proyecto

```
Data-Engineering-with-Databricks-Cookbook/
â”œâ”€â”€ Chapter01/          # Lectura de datos (CSV, JSON, Parquet, XML)
â”œâ”€â”€ Chapter02/          # Transformaciones bÃ¡sicas
â”œâ”€â”€ Chapter03/          # Delta Lake
â”œâ”€â”€ Chapter04/          # Streaming con Kafka
â”œâ”€â”€ Chapter05/          # Streaming avanzado
â”œâ”€â”€ Chapter06/          # OptimizaciÃ³n de Spark
â”œâ”€â”€ Chapter07/          # OptimizaciÃ³n de Delta
â”œâ”€â”€ Chapter08/          # Databricks Workflows
â”œâ”€â”€ Chapter09/          # Delta Live Tables
â”œâ”€â”€ Chapter10/          # Unity Catalog
â”œâ”€â”€ Chapter11/          # CI/CD y DevOps
â”œâ”€â”€ data/              # Datasets de ejemplo
â”œâ”€â”€ docker/            # ConfiguraciÃ³n de Docker
â”œâ”€â”€ docker-compose.yml # OrquestaciÃ³n de servicios
â””â”€â”€ build.sh          # Script de construcciÃ³n
```

## ğŸ”§ ConfiguraciÃ³n de Spark en JupyterLab

Cuando abras JupyterLab, puedes usar el siguiente cÃ³digo para conectarte al cluster Spark:

```python
from pyspark.sql import SparkSession

# Crear sesiÃ³n de Spark
spark = (SparkSession.builder
    .appName("mi-aplicacion")
    .master("spark://spark-master:7077")
    .config("spark.executor.memory", "512m")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate())

# Configurar nivel de logs
spark.sparkContext.setLogLevel("ERROR")

# Verificar conexiÃ³n
print(f"Spark Version: {spark.version}")
print(f"Spark Master: {spark.conf.get('spark.master')}")
```

## ğŸ“Š Datasets Incluidos

El proyecto incluye varios datasets de ejemplo en la carpeta `data/`:

- **Credit Card**: Datos de tarjetas de crÃ©dito para anÃ¡lisis de fraude
- **Netflix Titles**: CatÃ¡logo de tÃ­tulos de Netflix
- **Nobel Prizes**: Datos de premios Nobel
- **Partitioned Recipes**: Datos de recetas particionados por fecha

## ğŸ› ï¸ Comandos Ãštiles

### GestiÃ³n del Entorno

```bash
# Iniciar todos los servicios
docker-compose up -d

# Detener todos los servicios
docker-compose down

# Ver logs de un servicio especÃ­fico
docker-compose logs jupyterlab
docker-compose logs spark-master

# Reiniciar un servicio especÃ­fico
docker-compose restart jupyterlab

# Ver estado de todos los servicios
docker-compose ps
```

### Limpieza y Mantenimiento

```bash
# Limpiar contenedores e imÃ¡genes
docker system prune -a

# Eliminar volumen de datos compartidos
docker volume rm distributed-file-system

# Reconstruir imÃ¡genes desde cero
./build.sh
```

### Troubleshooting

```bash
# Ver logs detallados
docker-compose logs -f

# Acceder a un contenedor especÃ­fico
docker exec -it jupyterlab bash
docker exec -it spark-master bash

# Verificar conectividad entre servicios
docker exec -it jupyterlab ping spark-master
```

## ğŸš¨ SoluciÃ³n de Problemas Comunes

### 1. Puerto 8888 ya estÃ¡ en uso
```bash
# Cambiar puerto en docker-compose.yml
ports:
  - 8889:8888  # Cambiar 8888 por 8889
```

### 2. Error de memoria insuficiente
```bash
# Aumentar memoria en docker-compose.yml
environment:
  - SPARK_WORKER_MEMORY=1g  # Cambiar 512m por 1g
```

### 3. JupyterLab no se conecta a Spark
- Verificar que spark-master estÃ© ejecutÃ¡ndose: `docker-compose ps`
- Revisar logs: `docker-compose logs spark-master`
- Asegurarse de usar la URL correcta: `spark://spark-master:7077`

### 4. Problemas de permisos en Windows
```powershell
# Ejecutar PowerShell como administrador
# O configurar Docker Desktop para compartir el drive
```

## ğŸ“– PrÃ³ximos Pasos

1. **Explorar Chapter01**: Comienza con los notebooks de lectura de datos
2. **Practicar con datasets**: Usa los datos incluidos en la carpeta `data/`
3. **Experimentar con streaming**: Prueba los ejemplos de Kafka en Chapter04
4. **Optimizar performance**: Aprende tÃ©cnicas de optimizaciÃ³n en Chapter06-07

## ğŸ”— Recursos Adicionales

- [DocumentaciÃ³n de Apache Spark](https://spark.apache.org/docs/latest/)
- [Delta Lake Documentation](https://docs.delta.io/)
- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [JupyterLab Documentation](https://jupyterlab.readthedocs.io/)

## ğŸ“ Soporte

Si encuentras problemas:

1. Revisa los logs: `docker-compose logs`
2. Verifica que Docker tenga suficientes recursos asignados
3. AsegÃºrate de que los puertos no estÃ©n ocupados por otras aplicaciones
4. Revisa la documentaciÃ³n oficial de cada componente

---

**Â¡Disfruta aprendiendo Data Engineering con este entorno completo!** ğŸš€ 