#!/usr/bin/env python3
"""
Script de prueba rÃ¡pida para verificar que el entorno Spark estÃ© funcionando correctamente.
Ejecutar este script en JupyterLab para confirmar que todo estÃ¡ configurado correctamente.
"""

from pyspark.sql import SparkSession
import os

def test_spark_connection():
    """Prueba la conexiÃ³n al cluster Spark"""
    print("ğŸ” Probando conexiÃ³n al cluster Spark...")
    
    try:
        # Crear sesiÃ³n de Spark
        spark = (SparkSession.builder
            .appName("quick-test")
            .master("spark://spark-master:7077")
            .config("spark.executor.memory", "512m")
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
            .getOrCreate())
        
        # Configurar nivel de logs
        spark.sparkContext.setLogLevel("ERROR")
        
        print("âœ… ConexiÃ³n a Spark exitosa!")
        print(f"   â€¢ Spark Version: {spark.version}")
        print(f"   â€¢ Spark Master: {spark.conf.get('spark.master')}")
        print(f"   â€¢ Spark UI: http://localhost:4040")
        
        return spark
        
    except Exception as e:
        print(f"âŒ Error conectando a Spark: {e}")
        return None

def test_delta_lake(spark):
    """Prueba Delta Lake"""
    print("\nğŸ” Probando Delta Lake...")
    
    try:
        # Crear un DataFrame de prueba
        data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
        df = spark.createDataFrame(data, ["name", "age"])
        
        # Escribir como Delta
        df.write.format("delta").mode("overwrite").save("/tmp/test_delta")
        
        # Leer como Delta
        df_read = spark.read.format("delta").load("/tmp/test_delta")
        
        print("âœ… Delta Lake funcionando correctamente!")
        print(f"   â€¢ Registros escritos: {df.count()}")
        print(f"   â€¢ Registros leÃ­dos: {df_read.count()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error con Delta Lake: {e}")
        return False

def test_data_access(spark):
    """Prueba acceso a datos de ejemplo"""
    print("\nğŸ” Probando acceso a datos de ejemplo...")
    
    try:
        # Verificar si existen archivos de datos
        data_files = []
        if os.path.exists("/opt/workspace/data"):
            for root, dirs, files in os.walk("/opt/workspace/data"):
                for file in files:
                    if file.endswith(('.csv', '.json', '.parquet')):
                        data_files.append(os.path.join(root, file))
        
        if data_files:
            print("âœ… Archivos de datos encontrados:")
            for file in data_files[:5]:  # Mostrar solo los primeros 5
                print(f"   â€¢ {file}")
            if len(data_files) > 5:
                print(f"   â€¢ ... y {len(data_files) - 5} archivos mÃ¡s")
        else:
            print("âš ï¸  No se encontraron archivos de datos en /opt/workspace/data")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error accediendo a datos: {e}")
        return False

def test_kafka_connection():
    """Prueba conexiÃ³n a Kafka"""
    print("\nğŸ” Probando conexiÃ³n a Kafka...")
    
    try:
        from kafka import KafkaProducer, KafkaConsumer
        import json
        
        # Probar productor
        producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        # Enviar mensaje de prueba
        producer.send('test-topic', {'message': 'Hello Kafka!'})
        producer.flush()
        
        print("âœ… ConexiÃ³n a Kafka exitosa!")
        print("   â€¢ Productor funcionando")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error con Kafka: {e}")
        return False

def main():
    """FunciÃ³n principal de pruebas"""
    print("ğŸš€ Iniciando pruebas del entorno de Data Engineering...")
    print("=" * 60)
    
    # Prueba 1: ConexiÃ³n Spark
    spark = test_spark_connection()
    
    if spark:
        # Prueba 2: Delta Lake
        test_delta_lake(spark)
        
        # Prueba 3: Acceso a datos
        test_data_access(spark)
        
        # Prueba 4: Kafka
        test_kafka_connection()
        
        # Cerrar sesiÃ³n
        spark.stop()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Resumen de pruebas:")
    print("âœ… Spark Cluster: Funcionando")
    print("âœ… Delta Lake: Funcionando")
    print("âœ… Acceso a datos: Disponible")
    print("âœ… Kafka: Funcionando")
    print("\nğŸ‰ Â¡Tu entorno estÃ¡ listo para usar!")
    print("\nğŸ“± URLs importantes:")
    print("   â€¢ JupyterLab: http://localhost:8888")
    print("   â€¢ Spark Master: http://localhost:8080")
    print("   â€¢ Spark App UI: http://localhost:4040")

if __name__ == "__main__":
    main() 